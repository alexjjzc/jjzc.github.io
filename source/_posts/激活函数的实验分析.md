---
title: 激活函数的实验分析
date: 2022-06-05 10:15:03
index_img: /img/10.jpg
banner_img: /img/10.jpg
categories: Artificial Intelligence
tags: 激活函数
author: 禁忌之城
---
卷积神经网络中经常使用的激活函数有：Sigmoid函数、Tanh函数、ReLU函数、Softmax函数等，本文就关于这四类函数及其梯度图像进行对比分析，话不多说，上代码，看图！

<!--more-->

## 开始实验

### 1.Sigmoid函数

函数代码及图像：

``` python
import numpy as np
import matplotlib.pyplot as plt
def sigmoid(x):
    y=1/(1+np.exp(-x))
    return y
def plot_sigmoid():
    x=np.arange(-8,8,0.2)
    y=sigmoid(x)
    plt.plot(x,y)
    plt.show()
if __name__ == '__main__':
    plot_sigmoid()xxxxxxxxxx import numpy as npimport matplotlib.pyplot as pltdef sigmoid(x):    y=1/(1+np.exp(-x))    return ydef plot_sigmoid():    x=np.arange(-8,8,0.2)    y=sigmoid(x)    plt.plot(x,y)    plt.show()if __name__ == '__main__':    plot_sigmoid()$ hexo new "My New Post"bash
```

![](https://s2.loli.net/2022/06/05/z4kbL2DeAOviFKZ.png)

Sigmoid梯度函数代码及图像

```python
import numpy as np
import matplotlib.pyplot as plt
def plot_sigmoid():
    x = np.arange(-8, 8, 0.2)
    y = 1/(1+np.exp(-x))
    dy = y*(1-y)
    plt.plot(x, y, label='Sigmoid', linestyle='-', color='red')
    plt.plot(x, dy, label='Sigmoid derivative', linestyle='-', color='green')
    plt.legend(['Sigmoid', 'Sigmoid derivative'])
    plt.show()
if __name__ == '__main__':
    plot_sigmoid()
```

![](https://s2.loli.net/2022/06/05/DN9UHYWBrI7e5kM.png)

### 2.Tanh函数

函数代码及图像

``` python
import math
import numpy as np
import matplotlib.pyplot as plt
def plot_tanh():
    x = np.arange(-10, 10, 0.1)
    y = ((math.e ** (x) - math.e ** (-x)) / (math.e ** (x) + math.e ** (-x)))
    plt.xlim(-4, 4)
    plt.ylim(-1, 1.2)
    ax = plt.gca()
    ax.spines['right'].set_color('none')
    ax.spines['top'].set_color('none')
    ax.xaxis.set_ticks_position('bottom')
    ax.yaxis.set_ticks_position('left')
    ax.spines['bottom'].set_position(('data',0))
    ax.spines['left'].set_position(('data',0))
    plt.plot(x,y)
    plt.show()
if __name__ == '__main__':
    plot_tanh()
```

![](https://s2.loli.net/2022/06/05/LHYwfVR1SCpWGFN.png)

Tanh梯度函数代码及图像

```python
import numpy as np
import math
import matplotlib.pyplot as plt
def plot_tanh():
    x = np.arange(-10,10,0.1)
    y = ((math.e**(x)-math.e**(-x))/(math.e**(x)+math.e**(-x)))
    dy = 1-y*y
    plt.xlim(-4,4)
    plt.ylim(-1,1.2)
    ax = plt.gca()
    ax.spines['right'].set_color('none')
    ax.spines['top'].set_color('none')
    ax.xaxis.set_ticks_position('bottom')
    ax.yaxis.set_ticks_position('left')
    ax.spines['bottom'].set_position(('data',0))
    ax.spines['left'].set_position(('data',0))
    plt.plot(x,y, label='Tanh', linestyle='-', color='red')
    plt.plot(x,dy, label='Tanh derivative', linestyle='-', color='green')
    plt.legend(['Tanh','Tanh derivative'])
    plt.show()
if __name__ == "__main__":
    plot_tanh()
```

![](https://s2.loli.net/2022/06/05/oCKGuz8SbDgpnX7.png)

### 3.ReLU函数

函数代码及图像

```python
import numpy as np
import matplotlib.pyplot as plt
def relu(x):
    return np.maximum(0,x)
def plot_relu():
    x = np.arange(-5.0, 5.0, 0.1)
    plt.ylim([-1.0, 5.5])
    y = relu(x)
    plt.plot(x,y)
    plt.show()
if __name__ == '__main__':
    plot_relu()
```

![](https://s2.loli.net/2022/06/05/PJz3bokKDCWyFZO.png)

ReLU梯度函数代码及图像

```python
import numpy as np
import matplotlib.pyplot as plt
def plot_relu():
    x = np.arange(-2.5, 2.5, 0.1)
    plt.ylim([-0.5, 2.0])
    y = np.maximum(0,x)
    plt.plot(x,y,label='ReLU',color='red')

    x1 = np.linspace(-2,0)
    y1 = np.linspace(0,0)
    plt.plot(x1,y1,label='ReLU derivative',color='green')
    x2 = np.linspace(0,2)
    y2 = np.linspace(1,1)
    plt.plot(x2,y2,label='ReLU derivative',color='green')
    x3 = np.linspace(0,0)
    y3 = np.linspace(0,1)
    plt.plot(x3,y3,label='ReLU derivative',color='green')

    plt.legend(['Sigmoid', 'Sigmoid derivative'])
    plt.show()
if __name__ == '__main__':
    plot_relu()
```

![](https://s2.loli.net/2022/06/05/PXeWS85u1IZOGnd.png)

### 4.Softmax函数

Softmax函数多用于分类任务使用，故Softmax函数也可称为归一化指数函数，这是二分类函数Sigmoid函数在多分类上的推广，目的就是将多分类的结果以概率的形式展现出来。

Softmax函数的核心在于soft，与之相对应的是Hardmax函数，很多场景之下我们都需要找出数组所有元素中值最大的元素，实质上都是在求Hardmax函数。Hardmax函数最大的特点就是只选出其中一个最大的值，也即非黑即白，然而在实际应用中这种方式并不是很适用，比如对于文本分类，一篇文章中或多或少会包含着各种主题信息，我们更期望得到文章对于每个可能的文本类别的概率值（置信度），也可以理解为属于对应类别的可信度，所以此时就用到了Softmax函数，该函数的含义就在于不再唯一的确定某一个最大值，而是为每一个输出分类的结果都赋予一个概率值，表示属于每个类别的可能性。

Softmax函数可以将输出映射到0至1区间内，输出值累加为1，即输出值可看作是概率最大的值。由于是计算概率值，所以无法绘制函数图像。